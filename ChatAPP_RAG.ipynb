{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNplbPA29Dr4JYmWh0CsokR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VietHoangg03/ChatBot_RAG/blob/main/ChatAPP_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VhPx0GGQYbG",
        "outputId": "b2f9a0a3-959f-480e-bf3a-5b511f7bf7d4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit==1.46.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxIs40l5QkGf",
        "outputId": "b5274cb9-0f1f-4e88-80d1-69a581c9cdeb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.19.88.251"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-Tlt5z8QmFb",
        "outputId": "61abee24-6f21-4bf4-f79e-14b25e609346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.19.88.251:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://nine-hands-juggle.loca.lt\n",
            "2025-07-01 09:33:13.185216: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751362393.218381    8071 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751362393.229057    8071 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-01 09:33:13.262802: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "modules.json: 100% 229/229 [00:00<00:00, 1.27MB/s]\n",
            "config_sentence_transformers.json: 100% 123/123 [00:00<00:00, 754kB/s]\n",
            "README.md: 6.46kB [00:00, 20.2MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 289kB/s]\n",
            "config.json: 100% 777/777 [00:00<00:00, 5.00MB/s]\n",
            "model.safetensors: 100% 540M/540M [00:02<00:00, 255MB/s]\n",
            "tokenizer_config.json: 1.17kB [00:00, 5.57MB/s]\n",
            "vocab.txt: 895kB [00:00, 20.7MB/s]\n",
            "bpe.codes: 1.14MB [00:00, 99.3MB/s]\n",
            "added_tokens.json: 100% 22.0/22.0 [00:00<00:00, 123kB/s]\n",
            "special_tokens_map.json: 100% 167/167 [00:00<00:00, 1.12MB/s]\n",
            "config.json: 100% 270/270 [00:00<00:00, 1.34MB/s]\n",
            "config.json: 100% 615/615 [00:00<00:00, 4.34MB/s]\n",
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
            "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'cuda', 'mps', 'xpu', 'npu', 'hpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
            "\u001b[31m──\u001b[0m\u001b[31m────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────────\u001b[0m\u001b[31m──\u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mexec_code.py\u001b[0m: \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m128\u001b[0m in \u001b[92mexec_func_with_error_handling\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mscript_runner\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m669\u001b[0m in \u001b[92mcode_to_exec\u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m141\u001b[0m in \u001b[92m<module>\u001b[0m                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mst.write(answer)                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m139 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m140 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m141 \u001b[2m│   \u001b[0m\u001b[1;4mmain()\u001b[0m                                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m142 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m118\u001b[0m in \u001b[92mmain\u001b[0m                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m st.session_state.models_loaded:                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   \u001b[0mst.info(\u001b[33m\"\u001b[0m\u001b[33mĐang tải models...\u001b[0m\u001b[33m\"\u001b[0m)                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0mst.session_state.embeddings = load_embeddings()                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m118 \u001b[2m│   │   \u001b[0mst.session_state.llm = \u001b[1;4mload_llm()\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m│   │   \u001b[0mst.session_state.models_loaded = \u001b[94mTrue\u001b[0m                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m120 \u001b[0m\u001b[2m│   │   \u001b[0mst.success(\u001b[33m\"\u001b[0m\u001b[33mModels đã sẵn sàng!\u001b[0m\u001b[33m\"\u001b[0m)                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m│   │   \u001b[0mst.rerun()                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/caching/\u001b[0m\u001b[1;33mcache_utils.py\u001b[0m:\u001b[94m219\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m in \u001b[92m__call__\u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/caching/\u001b[0m\u001b[1;33mcache_utils.py\u001b[0m:\u001b[94m261\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m in \u001b[92m_get_or_create_cached_value\u001b[0m                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/caching/\u001b[0m\u001b[1;33mcache_utils.py\u001b[0m:\u001b[94m320\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m in \u001b[92m_handle_cache_miss\u001b[0m                                                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m44\u001b[0m in \u001b[92mload_llm\u001b[0m                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 41 \u001b[0m\u001b[2m│   \u001b[0m)                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 42 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 43 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Load model với quantization\u001b[0m                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 44 \u001b[2m│   \u001b[0mmodel = \u001b[1;4mAutoModelForCausalLM.from_pretrained(\u001b[0m                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 45 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mMODEL_NAME,\u001b[0m                                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 46 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mquantization_config=bnb_config,\u001b[0m                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 47 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4mdevice_map=\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mauto\u001b[0m\u001b[1;4;33m\"\u001b[0m                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/\u001b[0m\u001b[1;33mauto_factory.py\u001b[0m:\u001b[94m571\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m568 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_class = _get_model_class(config, \u001b[96mcls\u001b[0m._model_mapping)         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m569 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[0m\u001b[33mtext_config\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m570 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mconfig = config.get_text_config()                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m571 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mmodel_class.from_pretrained(\u001b[0m                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m572 \u001b[0m\u001b[1;2;4m│   │   │   │   \u001b[0m\u001b[1;4mpretrained_model_name_or_path, *model_args, config=config, **h\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m573 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m574 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m309\u001b[0m in        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_wrapper\u001b[0m                                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 306 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92m_wrapper\u001b[0m(*args, **kwargs):                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 307 \u001b[0m\u001b[2m│   │   \u001b[0mold_dtype = torch.get_default_dtype()                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 308 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 309 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 310 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfinally\u001b[0m:                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 311 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4mtorch.set_default_dtype(old_dtype)\u001b[0m                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 312 \u001b[0m                                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m4390\u001b[0m in       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4387 \u001b[0m\u001b[2m│   │   │   \u001b[0mhf_quantizer = \u001b[94mNone\u001b[0m                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4388 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4389 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m hf_quantizer \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m4390 \u001b[2m│   │   │   \u001b[0mhf_quantizer.validate_environment(                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4391 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtorch_dtype=torch_dtype,                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4392 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mfrom_tf=from_tf,                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m4393 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mfrom_flax=from_flax,                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/\u001b[0m\u001b[1;33mquantizer_bnb_4bit.p\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m84\u001b[0m in \u001b[92mvalidate_environment\u001b[0m                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 81 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfrom\u001b[0m\u001b[90m \u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mutils\u001b[0m\u001b[90m \u001b[0m\u001b[94mimport\u001b[0m is_bitsandbytes_multi_backend_available            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 82 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 83 \u001b[0m\u001b[2m│   │   \u001b[0mbnb_multibackend_is_enabled = is_bitsandbytes_multi_backend_available( \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 84 \u001b[2m│   │   \u001b[0m\u001b[1;4mvalidate_bnb_backend_availability(raise_exception=\u001b[0m\u001b[1;4;94mTrue\u001b[0m\u001b[1;4m)\u001b[0m                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 85 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 86 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m kwargs.get(\u001b[33m\"\u001b[0m\u001b[33mfrom_tf\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mFalse\u001b[0m) \u001b[95mor\u001b[0m kwargs.get(\u001b[33m\"\u001b[0m\u001b[33mfrom_flax\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mFalse\u001b[0m):     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 87 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/integrations/\u001b[0m\u001b[1;33mbitsandbytes.py\u001b[0m:\u001b[94m56\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m0\u001b[0m in \u001b[92mvalidate_bnb_backend_availability\u001b[0m                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m557 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[94mFalse\u001b[0m                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m558 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m559 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m is_bitsandbytes_multi_backend_available():                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m560 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4m_validate_bnb_multi_backend_availability(raise_exception)\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m561 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m _validate_bnb_cuda_backend_availability(raise_exception)            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m562 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/transformers/integrations/\u001b[0m\u001b[1;33mbitsandbytes.py\u001b[0m:\u001b[94m51\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m7\u001b[0m in \u001b[92m_validate_bnb_multi_backend_availability\u001b[0m                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m514 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m515 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m516 \u001b[0m\u001b[2m│   │   │   \u001b[0mlogger.error(err_msg)                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m517 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mRuntimeError\u001b[0m\u001b[1;4m(err_msg)\u001b[0m                                        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m518 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m519 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.warning(\u001b[33m\"\u001b[0m\u001b[33mNo supported devices found for bitsandbytes multi-back\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m520 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[94mFalse\u001b[0m                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[1;91mRuntimeError: \u001b[0m\u001b[3;35mNone\u001b[0m of the available devices `available_devices = \u001b[3;35mNone\u001b[0m` are supported by \n",
            "the bitsandbytes version you have installed: `bnb_supported_devices = \u001b[1m{\u001b[0m\u001b[32m'\"cpu\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mneeds an \u001b[0m\n",
            "\u001b[32mIntel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch \u001b[0m\n",
            "\u001b[32mversion\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'cuda'\u001b[0m, \u001b[32m'mps'\u001b[0m, \u001b[32m'xpu'\u001b[0m, \u001b[32m'npu'\u001b[0m, \u001b[32m'hpu'\u001b[0m\u001b[1m}\u001b[0m`. Please check the docs to see if the \n",
            "backend you intend to use is available and how to install it: \n",
            "\u001b[4;94mhttps://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers==4.52.4\n",
        "!pip install -q bitsandbytes==0.46.0\n",
        "!pip install -q accelerate==1.7.0\n",
        "!pip install -q langchain==0.3.25\n",
        "!pip install -q langchainhub==0.1.21\n",
        "!pip install -q langchain-chroma==0.2.4\n",
        "!pip install -q langchain_experimental==0.3.4\n",
        "!pip install -q langchain-community==0.3.24\n",
        "!pip install -q langchain_huggingface==0.2.0\n",
        "!pip install -q python-dotenv==1.1.0\n",
        "!pip install -q pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qnQK0aAUB3V",
        "outputId": "650ae75f-a52a-4b19-92ce-e0e5ce7a2151"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.3.26 requires langchain<1.0.0,>=0.3.26, but you have langchain 0.3.25 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import os\n",
        "import torch\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Session state initialization\n",
        "if 'rag_chain' not in st.session_state:\n",
        "    st.session_state.rag_chain = None\n",
        "if 'models_loaded' not in st.session_state:\n",
        "    st.session_state.models_loaded = False\n",
        "if 'embeddings' not in st.session_state:\n",
        "    st.session_state.embeddings = None\n",
        "if 'llm' not in st.session_state:\n",
        "    st.session_state.llm = None\n",
        "\n",
        "# Functions\n",
        "@st.cache_resource\n",
        "def load_embeddings():\n",
        "    return HuggingFaceEmbeddings(model_name=\"bkai-foundation-models/vietnamese-bi-encoder\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_llm():\n",
        "\n",
        "    MODEL_NAME = \"lmsys/vicuna-7b-v1.5\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,  # Hoặc load_in_8bit=True\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\"  # nf4 là lựa chọn tốt cho mô hình lớn\n",
        "    )\n",
        "\n",
        "    # Load model với quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    model_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    return HuggingFacePipeline(pipeline=model_pipeline)\n",
        "\n",
        "def process_pdf(uploaded_file):\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "        tmp_file.write(uploaded_file.getvalue())\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    loader = PyPDFLoader(tmp_file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    semantic_splitter = SemanticChunker(\n",
        "        embeddings=st.session_state.embeddings,\n",
        "        buffer_size=1,\n",
        "        breakpoint_threshold_type=\"percentile\",\n",
        "        breakpoint_threshold_amount=95,\n",
        "        min_chunk_size=500,\n",
        "        add_start_index=True\n",
        "    )\n",
        "\n",
        "    docs = semantic_splitter.split_documents(documents)\n",
        "    vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)\n",
        "    retriever = vector_db.as_retriever()\n",
        "\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | st.session_state.llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    os.unlink(tmp_file_path)\n",
        "    return rag_chain, len(docs)\n",
        "\n",
        "# UI\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"PDF RAG Assistant\", layout=\"wide\")\n",
        "    st.title(\"PDF RAG Assistant\")\n",
        "\n",
        "    st.markdown(\"\"\"\n",
        "    **Ứng dụng AI giúp bạn hỏi đáp trực tiếp với nội dung tài liệu PDF bằng tiếng Việt**\n",
        "\n",
        "    **Cách sử dụng đơn giản:**\n",
        "    1. **Upload PDF** → Chọn file PDF từ máy tính và nhấn \"Xử lý PDF\"\n",
        "    2. **Đặt câu hỏi** → Nhập câu hỏi về nội dung tài liệu và nhận câu trả lời ngay lập tức\n",
        "\n",
        "    ---\n",
        "    \"\"\")\n",
        "\n",
        "    # Load models\n",
        "    if not st.session_state.models_loaded:\n",
        "        st.info(\"Đang tải models...\")\n",
        "        st.session_state.embeddings = load_embeddings()\n",
        "        st.session_state.llm = load_llm()\n",
        "        st.session_state.models_loaded = True\n",
        "        st.success(\"Models đã sẵn sàng!\")\n",
        "        st.rerun()\n",
        "\n",
        "    # Upload PDF\n",
        "    uploaded_file = st.file_uploader(\"Upload file PDF\", type=\"pdf\")\n",
        "    if uploaded_file and st.button(\"Xử lý PDF\"):\n",
        "        with st.spinner(\"Đang xử lý...\"):\n",
        "            st.session_state.rag_chain, num_chunks = process_pdf(uploaded_file)\n",
        "            st.success(f\"Hoàn thành! {num_chunks} chunks\")\n",
        "\n",
        "    # Q&A\n",
        "    if st.session_state.rag_chain:\n",
        "        question = st.text_input(\"Đặt câu hỏi:\")\n",
        "        if question:\n",
        "            with st.spinner(\"Đang trả lời...\"):\n",
        "                output = st.session_state.rag_chain.invoke(question)\n",
        "                answer = output.split('Answer:')[1].strip() if 'Answer:' in output else output.strip()\n",
        "                st.write(\"**Trả lời:**\")\n",
        "                st.write(answer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uChO6JMCQmy_",
        "outputId": "a70a4ced-748e-4441-b51c-ec52e995de4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb98bc7f",
        "outputId": "94a618d4-2fed-4667-f82d-a904991fa44c"
      },
      "source": [
        "!pip install -q langchain-community"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    }
  ]
}